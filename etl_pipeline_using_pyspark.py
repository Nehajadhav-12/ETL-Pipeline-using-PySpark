# -*- coding: utf-8 -*-
"""ETL Pipeline using PySpark.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CswZ76_TjvxRhYYPIlyxdYQkPrv-IWb5

Step 1: Setting Up the Environment & Initializing a PySpark Session
"""

!pip install pyspark

"""Step 2: Extract – Load the Dataset

"""

from google.colab import files
upload = files.upload()

# load the CSV file into a Spark DataFrame
file_path = "temperature.csv"
df = spark.read.csv(file_path, header=True, inferSchema=True)

# display the schema and preview the data
df.printSchema()
df.show(5)

"""Step 3: Transform – Clean and Process the Data"""

# fill missing values for country codes
df = df.fillna({"ISO2": "Unknown"})

# drop rows where all temperature values are null
temperature_columns = [col for col in df.columns if col.startswith('F')]
df = df.dropna(subset=temperature_columns, how="all")

from pyspark.sql.functions import expr

# reshape temperature data to have 'Year' and 'Temperature' columns
df_pivot = df.selectExpr(
    "ObjectId", "Country", "ISO3",
    "stack(62, " +
    ",".join([f"'F{1961 + i}', F{1961 + i}" for i in range(62)]) +
    ") as (Year, Temperature)"
)

# convert 'Year' column to integer
df_pivot = df_pivot.withColumn("Year", expr("int(substring(Year, 2, 4))"))
df_pivot.show(5)

"""Step 4: Load – Save the Processed Data

"""

output_path = "/processed_temperature.parquet"
df_pivot.write.mode("overwrite").parquet(output_path)

# load the saved parquet file
processed_df = spark.read.parquet(output_path)
processed_df.show(5)